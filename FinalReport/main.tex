\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}

\setlength{\parindent}{0pt}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 27 Progress Report:\\Monocular Depth Perception}


\author{Omar Alam, Nirmal Chaudhari, Kevin Zhu \\
  \texttt{\{alamo2,chaudn12,zhuk41\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

As researchers are looking for more ways for computers to read the environment, depth perception is becoming an increasingly important feature. Traditionally, depth perception is done with LIDAR and radar. Lidar and radar-based sensors are accurate at long ranges but are more expensive compared to camera-based setups. Cameras, on the other hand do not have proper depth perception without the use of multiple lenses creating a stereo setup. Even then, stereo setups are less accurate than LiDAR or radar. 

To attempt the accuracy of LIDAR setups without needing use LIDAR, we are using deep learning techniques to create a model that can predict distances on an image. The model will be trained on taken images along with corresponding distances measured from more accurate LIDAR sources. This problem is still considered an unsolved research problem since it is extremely difficult for models to learn to estimate depth accurately enough for robotics applications using a single image only. 

If the single image depth perception is solved effectively with a robust solution, it will be impactful in many fields, with the primary use being for autonomous features in vehicles. As these vehicles need a large suite of sensors to properly detect their environment, its possible cut costs by reduce the number of sensors needed for proper autonomous function. Cheaper and more accessible depth perception would also encourage other companies to consider and integrate it into products.

\section{Dataset} \label{sec:dataset}

%You should write about your dataset here, following the guidelines 
%regarding item 1. This section may be 0.5-1 pages. Depending on your 
%specific dataset, you may want to include subsections for the preprocessing, 
%annotation, etc.

The monocular depth perception project is using the KITTI Depth Vision Benchmark Suite. 
This dataset contains over 93,000 gray scale depth maps along with corresponding
RGB images and LiDAR scans. The LiDAR scans provide accurate depth information 
which is used to generate the ground truth depth maps. The resolution of the RGB images
is 1242 x 375 pixels with 3 color channels for red, green, and blue. The depth maps
are single channel images with the same resolution as the RGB images, where each pixel
value represents the distance from the camera to the object in meters up to a 80 meter maximum.

The following images are a single example from the dataset.
\begin{figure}[H]
  \includegraphics[width=\columnwidth]{rgb_image_0.png}
  \caption{An example RGB image from the KITTI dataset.}
  \label{fig:kitti_rgb}
\end{figure}
\vspace{-20pt}
\begin{figure}[H]
  \includegraphics[width=\columnwidth]{depth_map_0-modified.png}
  \caption{The corresponding ground truth depth map for the RGB image in Figure~\ref{fig:kitti_rgb}. (Inverted for visualization purposes)}
  \label{fig:kitti_depth}
\end{figure}

The depth maps in the KITTI dataset have some missing values due 
to the limitations of the LiDAR sensor. The point clouds generated by 
the LiDAR tend to be sparse, especially for objects that are far away from the sensor.
Since the KITTI dataset also contains stereo images, the authors of the dataset
also use stereo reconstruction and semi-global matching. In their results,
they report achieving a better Mean Absolute Error (MAE) when combining LiDAR data
with stereo reconstruction, compared to using LiDAR data alone \cite{uhrig_2017_sparsity}.

The dataset may be downloaded from the official KITTI website\footnote{http://www.cvlibs.net/datasets/kitti/eval\_depth.php}
using their provided script and is already split into training and validation sets by the authors.
To retrieve the required dataset, the raw RGB images must be downloaded
separately from the depth maps. Some preprocessing was required to remove
RGB images that did not have corresponding depth maps. 

A preliminary analysis of 1000 images from the training set shows
the following depth distribution in the dataset:
\begin{figure}[H]
  \includegraphics[width=\columnwidth]{distance_distrib.png}
  \caption{Depth distribution of 1000 images from the KITTI dataset.}
  \label{fig:depth_distribution}
\end{figure}

As shown in Figure~\ref{fig:depth_distribution}, the depth values present
in the dataset are moderately skewed towards closer objects with frequency 
decreasing as depth increases.


\section{Features and Inputs} \label{sec:features}

% TODO: Need to update

For the implementation, no feature engineering or feature selection is being done. The model simply takes in an RGB image as input and processes it through a ResNet Convolutional Neural Network backbone. This transforms the pixels data into learned feature embeddings space. As such, since the backbone is an off-the-shelf CNN that learns feature representation directly from the incoming data, there is no manual feature selection needed. The exact implementation details of the backbone into the overall Neural Network model will be described in the section \href{sec:implementation}{below}. 

The use of a ResNet CNN backbone is similar to what other related works like DCDepth, MSFusion and NDDepth did as described in \href{sec:features}{section 2}, where features and correlations are dynamically extracted. The reason this works well, as opposed to manually selecting features is because the correlations between various features change a lot based on the image that is presented. As such, we need a model that captures geometric and semantic cues that are hard to generalize as hard-coded features. Evidently a CNN is needed to extract the features themselves. 

The ResNet CNN backbone and depth neural network are both a part of the same neural network, but separated as stages in the overall structure. The CNN backbone serves as the "encoder" stage of the overall neural network, where it is a CNN that outputs the learned feature embeddings space. These features are then passed into the "decoder" layer which upsamples the embeddings to return a depth map that is the same size as the original RGB image.

\section{Implementation} \label{sec:implementation}

% TODO: Need to complete

\section{Evaluation} \label{sec:results_and_evaluation}

%How are you evaluating your model? What results do 
%you have so far? What are your baselines? Refer to item 5. 
%This may take around 0.5 pages.
% TODO: NEED TO UPDATE

The loss function used to train the model is the Scale Invariant Loss, which was first defined 
by Eigen et al. in their 2014 paper "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network" \cite{eigen_2014_depth}.

It is defined as follows:
\[
  D(y, y^*) = \frac{1}{n} \sum_{i=0}^{n} d_{i}^2 - \frac{1}{n^2} (\sum_{i=0}^{n} d_i)^2
\]

Where $d_i = \log y_i - \log y_i^*$, $y_i$ is the predicted depth, $y_i^*$ is the ground truth depth, and $n$ is the number of pixels in the depth map.

This function is particularly useful for evaluating the quality of the depth estimation 
since it focuses on the relative differences between predicted and ground truth depths,
rather than absolute differences. This is important since depth predictions can vary
by a scale factor as long as the relative distances between objects are preserved.
Ex: If the predicted depth is consistently twice the ground truth depth, the scale invariant loss will be low, indicating that the model has learned
the relative depth relationships correctly.

In addition to using SiLog, the following metrics are also used:
\begin{enumerate}
  \item Absolute Relative Difference (Abs Rel):
  \[
    \text{Abs Rel} = \frac{1}{n} \sum_{i=0}^{n} \frac{|y_i - y_i^*|}{y_i^*}
  \]

  \item Relative Squared Difference (Rel Sq):
  \[
    \text{Rel Sq} = \frac{1}{n} \sum_{i=0}^{n} \frac{(y_i - y_i^*)^2}{(y_i^*)^2}
  \]

  \item Inverse Root Mean Squared Error (iRMSE):
  \[
    \text{iRMSE} = \sqrt{\frac{1}{n} \sum_{i=0}^{n} \left(\frac{1}{y_i} - \frac{1}{y_i^*}\right)^2}
  \]
\end{enumerate}
These metrics provide a comprehensive evaluation of the model's performance.
The iRMSE metric is particularly useful for determining depth accuracy for closer objects.
It penalizes large errors in depth estimation for nearby objects more heavily than for distant objects,
since it is based on the inverse of depth values.

The dataset is already split into training and validation sets by the authors of the KITTI dataset as 
mentioned in the \href{sec:dataset}{Dataset} section above with a roughly 80/20 split.
There is no cross-validation performed due to the sufficiently large size of the dataset.
However, this experiment was conducted with a 
very small subset of the entire dataset, using only 1000 images for training due to 
computational constraints with the following results over 50 epochs:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Validation Loss (SiLog) & 0.3224 \\
Training Loss (SiLog) & 0.5668 \\
Rel Sq & 4.2883 \\
Abs Rel & 0.4022 \\
iRMSE & 0.0425 \\
\hline

\end{tabular}
\caption{Model Evaluation Metrics on Validation Set after 50 Epochs}
\label{tab:metrics}
\end{table}

\section{Progress} \label{sec:progress}

% TODO: Need to complete

\section{Error Analysis} \label{sec:error_analysis}

% TODO: Need to complete

\section*{Team Contributions} \label{sec:team_contributions}
Everyone in the group contributed equally to this project. 
Omar Alam focused on the dataset research and preprocessing, 
Nirmal Chaudhary focused on the implementation of the model, 
and Kevin Zhu focused on researching related works. 
Everyone contributed to writing the report and editing.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only

\bibliography{custom}


% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
