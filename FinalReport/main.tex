\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called
% camera-ready) version. Change to "preprint" to generate a non-anonymous
% version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
% For proper rendering and hyphenation of words containing Latin characters
% (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters \usepackage[T5]{fontenc} See
% https://www.latex-project.org/help/documentation/encguide.pdf for other
% character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{listings-ext}
\usepackage{amsmath}

\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{mystyle}{ backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen}, keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray}, stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize, breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false, showtabs=false,                  
    tabsize=2 }

\lstset{style=mystyle}

\setlength{\parindent}{0pt}

% If the title and author information does not fit in the area allocated,
% uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 27 Final Report:\\Monocular Depth Perception}


\author{Omar Alam, Nirmal Chaudhari, Kevin Zhu \\
  \texttt{\{alamo2,chaudn12,zhuk41\}@mcmaster.ca} \\  
  \href{https://github.com/alamo3/SFWRENG-4AL3-Group-27}{GitHub}
  }


\begin{document}
\maketitle

% \begin{abstract} \end{abstract}

\section{Introduction}

As researchers are looking for more ways for computers to read the environment,
depth perception is becoming an increasingly important feature. Traditionally,
depth perception is done with LIDAR and radar. Lidar and radar-based sensors are
accurate at long ranges but are more expensive compared to camera-based setups.
Cameras, on the other hand do not have proper depth perception without the use
of multiple lenses creating a stereo setup. Even then, stereo setups are less
accurate than LiDAR or radar. 

To attempt the accuracy of LIDAR setups without needing use LIDAR, we are using
deep learning techniques to create a model that can predict distances on an
image. The model will be trained on taken images along with corresponding
distances measured from more accurate LIDAR sources. This problem is still
considered an unsolved research problem since it is extremely difficult for
models to learn to estimate depth accurately enough for robotics applications
using a single image only. 

If the single image depth perception is solved effectively with a robust
solution, it will be impactful in many fields, with the primary use being for
autonomous features in vehicles. As these vehicles need a large suite of sensors
to properly detect their environment, its possible cut costs by reduce the
number of sensors needed for proper autonomous function. Cheaper and more
accessible depth perception would also encourage other companies to consider and
integrate it into products.

There are numerous groups of people and institutions that experimented with
unique methods for monocular depth prediction, particularly using the same KITTI
dataset described in the section \href{sec:dataset}{below}. In fact, on the
website that was used to retrieve this dataset, there is a leaderboard outlining
which methods yield the best results \citep{KITTI}. These methods were explored
by our team to understand the approaches that obtain the best result. Although
the related works were used as inspiration, the purpose of this project is to
experiment and find unique ways to improve on existing methods. 

While looking into related work, we realized some approaches were training a
model to analyze stereo depth perception instead of monocular. These approaches
were ignored for the purpose of this project since the focus is on monocular
depth perception. But the comparison of backbones used between stereo versus
monocular approaches was interesting. Since, the initial assumption was that for
monocular depth perception a more expressive backbone would be needed since it
can't use parallax method. However, this wasn't really mentioned in any of the
papers. 

For the initial implementation, we decided to focus on approaches that use a CNN
to extract features before passing them into a Neural Network. Namely, the
MSFusion, DCDepth and NDDepth approaches. The UniDepth approach sparked
inspiration to look into using a transformer later on for improvement. 
\begin{itemize}
  \item UniDepth: Uses a ViT-based DPT (Dense Prediction Transformer) backbone.
  For the decoder, it uses a DPT-style multi-scale fusion decoder. For the depth
  representation, it predicts a pseudo-spherical 3D representation.
  \citep{Piccinelli2024UniDepth}. 
  \item MSFusion: Uses a standard ResNet50-based encoder backbone to extract 2D
  texture features. It also converts the image into a 3D point cloud
  representation. It then uses a multi-space fusion decoder to fuse both into a
  spatial feature map. For depth representation, it outputs the actual depth
  prediction \citep{Bie2025MultiSpaceDepth}.
  \item DCDepth: Uses a standard ResNet or HRNet backbone to extract 2D image
  features from RGB image. The decoder outputs DCT frequency coefficients for
  each depth patch. Uses inverse DCT to transform the coefficients into a
  pixel-wise depth as the depth representation \citep{Wang2024DCDepth}.
  \item NDDepth: Uses a standard ResNet-based backbone to extract 2D image
  features from RGB image. The decoder branches into two prediction heads, the
  normal distance head and the direct depth head. The depth representation is
  the true depth in meters, which is derived by selecting which head to trust
  per pixel \citep{Shao2023NDDepth}.
  \item PixelFormer: The encoder here extracts multi-scale image features. The
  decoder then takes these pixel-level features and progressively increases
  spatial resolution using Skip Attention Module. To predict depth bins, it
  performs ordinal regression. \citep{Agarwal2023Attention}
  \item DepthFormer: Uses dual-branch or parallel encoders, consisting of a
  transformer branch and CNN branch. Transformer handles global context, and
  long-range correlation. Convolution branch preserves local information. The
  decoder is then a neural network that maps the two feature sets into a depth
  map \citep{Li2023DepthFormer}
\end{itemize}

\subsection*{Post Progress Report}

For the initial implementation, we mainly focused on the MSFusion, DCDepth and
NDDepth approaches, which all use a ResNet backbone. However, since the accuracy
of our model was a lot worse than the other approaches, we knew we had to make a
change. 

After the progress report, the PixelFormer and DepthFormer projects made us look
into using transformers as the backbone to extract features instead of a CNN to
extract the features. After reading those papers, we realized vision
transformers are able to capture long-range dependencies without needing many
convolutional layers. The DepthFormer article was particularly interesting since
it involved extracting features using CNN and transformers in parallel. Allowing
us to capture fine-grained and long-range features simultaneously. However, we
didn't have enough time to explore that, so we decided to just look at how
vision transformers compared to using CNN as the backbone. 

\section{Dataset} \label{sec:dataset}

%You should write about your dataset here, following the guidelines regarding
%item 1. This section may be 0.5-1 pages. Depending on your specific dataset,
%you may want to include subsections for the preprocessing, annotation, etc.

The monocular depth perception project is using the KITTI Depth Vision Benchmark
Suite. This dataset contains over 93,000 gray scale depth maps along with
corresponding RGB images and LiDAR scans. The LiDAR scans provide accurate depth
information which is used to generate the ground truth depth maps. The
resolution of the RGB images is 1242 x 375 pixels with 3 color channels for red,
green, and blue. The depth maps are single channel images with the same
resolution as the RGB images, where each pixel value represents the distance
from the camera to the object in meters up to a 80 meter maximum.

The following images are a single example from the dataset.
\begin{figure}[H]
  \includegraphics[width=\columnwidth]{rgb_image_0.png}
  \caption{An example RGB image from the KITTI dataset.}
  \label{fig:kitti_rgb}
\end{figure}
\vspace{-20pt}
\begin{figure}[H]
  \includegraphics[width=\columnwidth]{depth_map_0-modified.png}
  \caption{The corresponding ground truth depth map for the RGB image in
  Figure~\ref{fig:kitti_rgb}. (Inverted for visualization purposes)}
  \label{fig:kitti_depth}
\end{figure}

The depth maps in the KITTI dataset have some missing values due to the
limitations of the LiDAR sensor. The point clouds generated by the LiDAR tend to
be sparse, especially for objects that are far away from the sensor. Since the
KITTI dataset also contains stereo images, the authors of the dataset also use
stereo reconstruction and semi-global matching. In their results, they report
achieving a better Mean Absolute Error (MAE) when combining LiDAR data with
stereo reconstruction, compared to using LiDAR data alone
\cite{uhrig_2017_sparsity}.

The dataset may be downloaded from the official KITTI
website\footnote{http://www.cvlibs.net/datasets/kitti/eval\_depth.php} using
their provided script and is already split into training and validation sets by
the authors. To retrieve the required dataset, the raw RGB images must be
downloaded separately from the depth maps. Some preprocessing was required to
remove RGB images that did not have corresponding depth maps. 

A preliminary analysis of 1000 images from the training set shows the following
depth distribution in the dataset:
\begin{figure}[H]
  \includegraphics[width=\columnwidth]{distance_distrib.png}
  \caption{Depth distribution of 1000 images from the KITTI dataset.}
  \label{fig:depth_distribution}
\end{figure}

As shown in Figure~\ref{fig:depth_distribution}, the depth values present in the
dataset are moderately skewed towards closer objects with frequency decreasing
as depth increases.

\subsection*{Post Progress Report}

For the initial implementation, we were only using a very small subset of the
KITTI dataset we mentioned. To improve the overall accuracy, we decided on using
the full dataset, which was at least 20 times larger. This also ensured that our
benchmark was being compared against accurately, since it doesn't make sense to
compare a model that was trained on a subset of the dataset against a model that
was trained on the full dataset. As such, now that we used the entire dataset,
we can compare more confidently against the other approaches. 

\section{Features and Inputs} \label{sec:features}

For the implementation, no feature engineering or feature selection is being
done. The models trained through the course of the project simply take in
normalized RGB images as input and a ground truth depth map during training. The
Resnet and Vision Transformer models consume the pixel data into learned feature
embeddings space. In both of the Resnet and transformer prediction
architectures, these extracted features are passed to a decoder network. No
other pre-processing or feature engineering is applied to the input datsaset.

The exact implementation of these architectures will be detailed in the
\href{sec:implementation}{ implementation } section.


\section{Implementation} \label{sec:implementation}

\subsection{DepthNet (ResNet Backbone)}

Two different sizes of the ResNet model (34 and 50) were used to form the
feature extracting backbone of the DepthNet implementation. Pretrained weights
of ResNet 34 and 50 are easily available through the models module in PyTorch.
For this project, the model weights pretrained on the ImageNet-1K dataset were
loaded. This was decided upon, rather than training the entire model for
multiple reasons:
\begin{itemize}
  \item The pretrained model weights are determined from training over 1 million
  images and classifying 1000 classes. 
  \item Pretrained models provide a very good baseline to fine tune off of since
  they already understand images well.
  \item Training from scratch to achieve significant accuracy would require much
  more than the 93,000 images available in the dataset.
  \item Resource and hardware limitations prevent the project from training on
  millions of images.
\end{itemize}

The outputs from specific layers of the ResNet models is extracted by
referencing the layers within the model and executing them standalone during the
forward pass of our DepthNet implementation. 

\begin{lstlisting}[language=Python]
# Encoder: ResNet-34 resnet = models.resnet50(weights=
models.ResNet50_Weights.IMAGENET1K_V1)

self.conv1 = resnet.conv1   # (64 channels, /2) self.bn1   = resnet.bn1
self.relu  = resnet.relu self.maxpool = resnet.maxpool

self.layer1 = resnet.layer1   # 64 channels  (/4) self.layer2 = resnet.layer2
# 128 channels (/8) self.layer3 = resnet.layer3   # 256 channels (/16)
self.layer4 = resnet.layer4   # 512 channels (/32)
\end{lstlisting}

The decoder architecture is the same regardless of the backbone being utilized
for DepthNet.

The decoder consists of multiple convolutional layers which progressively
upsample the features extracted from the ResNet layers.

\begin{lstlisting}[language=Python]
def _upsample_block(self, in_channels, out_channels): return nn.Sequential(
    nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),
    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True) )
\end{lstlisting}

This layer is used in the following configuration:
\begin{lstlisting}[language=Python]
# Decoder self.up4 = self._upsample_block(2048, 1024)  # /16 self.up3 =
self._upsample_block(1024, 512)  # /8 self.up2 = self._upsample_block(512, 256)
# /4 self.up1 = self._upsample_block(256, 64)    # /2

# Final depth prediction layer self.final = nn.Conv2d(64, 1, kernel_size=3,
padding=1)
\end{lstlisting}

Each upsampling block upsamples the input received by reducing the output
resolution in half up to the final depth prediction convolution kernel which
upsamples the 64 channels to a 1 channel depth map. 

As explained in the \href{sec:results_and_evaluation}{evaluation} section, the
results from using either of these backbones and the decoder architecture
described are very similar. This suggests that the jump from 34 to 50 layers did
not significantly improve the pretrained model's ability to distinguish features
critical to depth. If this was the case, the weights of the layers extracted
from the ResNet-50 model should yield a lower error which they do not. 

\subsection{Vision Transformer Depth Net}

The following implementation is heavily influenced by the architecture described
by Ranftl. et al \cite{Ranftl-2021}.

\vspace{10pt}
For the vision transformer depth net architecture, a pretrained vision
transformer is utilized to further fine tune. This is necessary due to the
reasons mentioned for the ResNet50 models, with the added constraint that
transformer architectures need millions (if not hundreds of millions) of
examples to learn from. This is because convolutional networks possess an
intrinsic ability to learn shapes and edges as they train which the
self-attention mechanism does not. 

For this DepthNet, we load the pretrained vision transformer from PyTorch which
has been trained on ImageNet-1K and utilizes 16x16 patches of the original image
as input tokens.

The pretrained model consists of 12 attention layers, with each successive layer
focusing on finer details of the image patches. We can do feature extraction
from this transformer model by executing some of the attention layers in the
forward pass of the DepthNet. 

\begin{lstlisting}[language=Python]
# We will save the data from these intermediate layers # to pass to the decoder.
# This base model is composed of 12 transformer layers. self.intermediate_layers
= [2, 5, 8, 11]

def forward(self, x): .... features = [] for i, block in
enumerate(self.model.blocks): x = block(x) if i in self.intermediate_layers:
features.append(x) .....
\end{lstlisting}

The raw output of the attention layers consists of 1D tokens representing
features. These features need to be reconstructed into 2D maps that can be later
used to predict the depth map. To perform this reconstruction, a "Reassemble"
block was developed to convert the 1D vector into a series of 2D patches with
upscaling applied as necessary. 

\begin{lstlisting}[language=Python]
class ReassembleBlock(torch.nn.Module): def __init__(self, in_channels,
    out_channels, scale_factor): .....

      self.project = nn.Conv2d(in_channels, out_channels, kernel_size=1)

      if scale_factor != 1.0: self.resize =
        nn.Upsample(scale_factor=scale_factor, mode="bilinear",
        align_corners=False) else: self.resize = nn.Identity()

      # Paper refines the output afterwards with a convolution self.conv =
        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1) ......
\end{lstlisting}

This is paired with a Fusion Block to fuse higher and lower resolution features
from different layers.

\begin{lstlisting}[language=Python]
x_lower_upsampled = F.interpolate(x_lower_res, size = x_higher_res.shape[-2:],
mode = "bilinear" , align_corners = False)

out = x_higher_res + x_lower_upsampled

residual = out

# This is a standard residual block from resnet. out = self.conv1(out) out =
self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out)

out += residual # We add the input back to the output.

out = self.relu(out)
  
\end{lstlisting}

These blocks in combination with the vision transformer define the architecture
for the DensePredictionNetwork DepthNet. 

\begin{lstlisting}[language=Python]

B, C, H, W = x.shape

# Output from vision transformer layers tokens_list = self.encoder(x)

f1 = self.reassemble_1(tokens_list[0], H, W) f2 =
self.reassemble_2(tokens_list[1], H, W) f3 = self.reassemble_3(tokens_list[2],
H, W) f4 = self.reassemble_4(tokens_list[3], H, W)

out = self.fusion_1(f3, f4) out = self.fusion_2(f2, out) out = self.fusion_3(f1,
out)

depth = self.depth_prediction_head(out)

depth = F.interpolate(depth, size=(H, W), mode="bilinear", align_corners=False)
\end{lstlisting}

The depth prediction head uses a similar upsampling process to the ResNet based
DepthNet.

\begin{lstlisting}[language=Python]
self.depth_prediction_head = nn.Sequential( nn.Conv2d(self.transformer_features,
  self.transformer_features // 2, kernel_size=3, padding=1),
  nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False),
  nn.Conv2d(self.transformer_features // 2, 32, kernel_size=3, padding=1),
  nn.ReLU(inplace=True), nn.Conv2d(32, 1, kernel_size=1, padding=1),
  nn.ReLU(inplace=True) )  
\end{lstlisting}

Self-attention mechanisms correlate relationships across the input context much
better than convolutional layers and this major difference will be reflected in
the \href{sec:results_and_evaluation}{results} section.

\subsection{Baseline Models}

To help evaluate our model we decided create some baseline models. The first baseline model created was a model that would return a linear random value between 0 and 1 representing the minimum and maximum depth possible at each pixel. The second baseline model always returned only zeros, assuming that the depth will always be as close as possible. As the depth distribution favours numbers closer to zero this should have better results than the fully random model and works as a better baseline.

Other usable baselines can be found on the KITTI website and can be used to compare our method to other methods that have been published with accompanying papers, results and environments. Note that the numbers on this site are multiplied by compared to the values we are using. 


\subsection{Engineering Challenges}
To successfully train the transformer DepthNet, significant 
engineering challenges needed to be resolved. Training such a 
large model would not be feasible on any of the group's computers.
This led to the usage of the AdaX cluster provided by the CAS department
at McMaster University. This cluster consists of 8 L40S Nvidia GPUs with
48 Gigabytes of VRAM in each GPU. To harness the power of all the GPUs
available, parallel training was utilized. This was accomplished by using
the "Accelerate" Python library. This Hugging Face library splits the PyTorch
training across multiple different processes and aggregates the results
to perform backwards propagation. This yielded significant improvement
in training times and results when compared to using a single GPU or a 
personal computer. 




\section{Evaluation} \label{sec:results_and_evaluation}

%How are you evaluating your model? What results do you have so far? What are
%your baselines? Refer to item 5. This may take around 0.5 pages. TODO: NEED TO
%UPDATE

The loss function used to train the model is the Scale Invariant Loss, which was
first defined by Eigen et al. in their 2014 paper "Depth Map Prediction from a
Single Image using a Multi-Scale Deep Network" \cite{eigen_2014_depth}.

It is defined as follows:
\[
  D(y, y^*) = \frac{1}{n} \sum_{i=0}^{n} d_{i}^2 - \frac{1}{n^2} (\sum_{i=0}^{n} d_i)^2
\]

Where $d_i = \log y_i - \log y_i^*$, $y_i$ is the predicted depth, $y_i^*$ is
the ground truth depth, and $n$ is the number of pixels in the depth map.

This function is particularly useful for evaluating the quality of the depth
estimation since it focuses on the relative differences between predicted and
ground truth depths, rather than absolute differences. This is important since
depth predictions can vary by a scale factor as long as the relative distances
between objects are preserved. Ex: If the predicted depth is consistently twice
the ground truth depth, the scale invariant loss will be low, indicating that
the model has learned the relative depth relationships correctly.

In addition to using SiLog, the following metrics are also used:
\begin{enumerate}
  \item Absolute Relative Difference (Abs Rel):
  \[
    \text{Abs Rel} = \frac{1}{n} \sum_{i=0}^{n} \frac{|y_i - y_i^*|}{y_i^*}
  \]

  \item Relative Squared Difference (Rel Sq):
  \[
    \text{Rel Sq} = \frac{1}{n} \sum_{i=0}^{n} \frac{(y_i - y_i^*)^2}{(y_i^*)^2}
  \]

  \item Inverse Root Mean Squared Error (iRMSE):
  \[
    \text{iRMSE} = \sqrt{\frac{1}{n} \sum_{i=0}^{n} \left(\frac{1}{y_i} - \frac{1}{y_i^*}\right)^2}
  \]
\end{enumerate}
These metrics provide a comprehensive evaluation of the model's performance. The
iRMSE metric is particularly useful for determining depth accuracy for closer
objects. It penalizes large errors in depth estimation for nearby objects more
heavily than for distant objects, since it is based on the inverse of depth
values.

The dataset is already split into training and validation sets by the authors of
the KITTI dataset as mentioned in the \href{sec:dataset}{Dataset} section above
with a roughly 80/20 split. There is no cross-validation performed due to the
sufficiently large size of the dataset.

\subsection*{Initial Implementation Evaluation}
This experiment was conducted with a very small subset of the entire dataset,
using only 1000 images for training due to computational constraints with the
following results over 50 epochs. This implementation was the result of using a
ResNet34 backbone to extract the features.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Validation Loss (SiLog) & 0.3224 \\
Training Loss (SiLog) & 0.5668 \\
Rel Sq & 4.2883 \\
Abs Rel & 0.4022 \\
iRMSE & 0.0425 \\
\hline

\end{tabular}
\caption{ResNet34 Model Evaluation Metrics on Validation Set after 50 Epochs}
\label{tab:metrics}
\end{table}

\begin{figure}[H]
  \includegraphics[width=\columnwidth]{resnet34.png}
  \caption{Resnet34 Model Comparison}
  \label{fig:resnet34_comparison}
\end{figure}

\subsection*{ResNet50 Implementation Evaluation}
This experiment was conducted with the entire dataset this time, with a change
in the backbone being used. Instead of using a Resnet34 backbone, we decided on
using a ResNet50 backbone, since that's the backbone that is used in the
approaches we saw. And also it is a wider and deeper CNN compared to the
ResNet34. 

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Validation Loss (SiLog) & 0.3404 \\
Training Loss (SiLog) & 0.2315 \\
Rel Sq & 4.5360 \\
Abs Rel & 0.3832 \\
iRMSE & 0.0455 \\
\hline

\end{tabular}
\caption{ResNet50 Model Evaluation Metrics on Validation Set after 50 Epochs}
\label{tab:metrics}
\end{table}

\begin{figure}[H]
  \includegraphics[width=\columnwidth]{resnet50.png}
  \caption{Resnet50 Model Comparison}
  \label{fig:resnet50_comparison}
\end{figure}

The results are very similar to just using the ResNet34 backbone. Perhaps since
it was trained on a much larger dataset compared to the ResNet34 model, it has a
lower accuracy, since it is unable to accurately fit and learn all the
datapoints. 

\subsection*{Vision Transformer Implementation Evaluation}

This evaluation is for the vision transformer, which has been described in the
Implementation section above. 

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Validation Loss (SiLog) & 0.0033815 \\
Training Loss (SiLog) & 0.001112 \\
Rel Sq & 0.09019 \\
Abs Rel & 0.03577 \\
iRMSE & 0.004587 \\
\hline

\end{tabular}
\caption{Vision Transformer Model Evaluation Metrics on Validation Set after 50 Epochs}
\label{tab:metrics}
\end{table}

\begin{figure}[H]
  \includegraphics[width=\columnwidth]{transformer.png}
  \caption{Transformer Model Comparison (Input, Prediction, Ground Truth)}
  \label{fig:transformer_comparison}
\end{figure}

These results show a much higher accuracy compared to the ResNet50 model, which
can also visually be seen when comparing Figure 6 against Figure 5.
It is also interesting to note that the vehicles predicted in the depth
map by the transformer have very LiDAR scan points in the ground truth data 
as they show up as empty spaces. The model impressively predicts the vehicles
being there based on the image alone and incorporates them into the depth map.
This result is technically "better" than the ground truth. This could lead to
unforeseen penalization of the model by the loss function. This may be avoided in
the future by using better datasets. 

\subsection*{Baseline Evaluation}
The evaluation of the linearly random baseline, which has been described above.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Validation Loss (SiLog) & 1.3207 \\
Rel Sq & 8.4483 \\
Abs Rel & 0.6587 \\
iRMSE & 1.9262 \\
\hline

\end{tabular}
\caption{Linear Random Baseline Evaluation Metrics }
\label{tab:metrics}
\end{table}

The evaluation of the zero baseline which always returns zero depth.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Validation Loss (SiLog) & 0.3345 \\
Rel Sq & 4.4797 \\
Abs Rel & 0.4094 \\
iRMSE & 0.0432 \\
\hline

\end{tabular}
\caption{Zero Baseline Evaluation Metrics }
\label{tab:metrics}
\end{table}

The results of the baseline show that our initial attempts, while better than a random depth map are not good models as they are comparable to the zero baseline. However, the vision transformer model is not only better than our initial attempts it is also much better than the baselines, meaning that the model is properly evaluating depth as compared to the previous models. 

Comparing our values to the benchmarks on the KITTI website gives us similar results as our benchmarks. Our initial models were bad and had a SILog of almost 2x the worst model published on the website. However, the vision transformer model did much better with a SiLog of 0.0033815 or 0.33815 as would be displayed on the website.

\section{Progress} \label{sec:progress}

For the final implementation, we stayed on track with what we had planned in the
progress report. Although we used a ResNet34 model in the initial
implementation, we always planned to first experiment with using a ResNet50 and
then a Vision Transformer backbone. Both of these modifications were inspired by
the approaches we saw in the papers listed in the Introduction section. \\\\
This being said, we were quite surprised when the ResNet50 backbone model
performed equally well to the Resnet34 backbone model despite the backbone being
wider and deeper. This was one aspect that made us look into more papers like
the DepthFormer to see what advantages CNN backbones have against transformer
backbones. \\\\
Moreover, we also followed through with our goal of using the entire KITTI
dataset, instead of just using a subset to ensure we can confidently compare our
model against the ones listed in the approaches section of the KITTI website. 

\section{Error Analysis} \label{sec:error_analysis}

The error was analyzed by comparing the 5 metrics outlined in the Evaluation
section against the benchmarks on the KITTI website. \\\\
Each metric defines a unique error that outlines what the model is doing well or
poorly in. We used these metrics to determine what the next change we should
make is.

\subsection*{SiLog - Scale Invariant Log Loss}

This is one of the main errors we are using to determine how well the model is
performing since it gives a key piece of information; is the model overfitting
the dataset? Measures the difference between predicted and true depth in the
log-space. This makes it less sensitive to absolute depth scale. \\\\
Since we are working with monotonic depth perception here, it is important to
compare the loss from a relative depth accuracy space, since we are less
concerned with the true depth value, and more concerned with how the depth of
each pixel compares with the ones around it. \\\\
We are actually using this metric to measure the validation and training loss,
and hence this metric is being directly used to train the model itself. \\\\
To systematically determine whether the model is performing well in this case,
the key thing we look at is whether or not the model is overfitting or
underfitting the dataset. In the case of the Transformer Model, it doesn't
appear to be doing that. However, there is a much larger gap between validation
and training loss for ResNet34 and ResNet50, which implies there is some
overfitting happening there. As such, it's possible the ResNet50 model is
outputting features that act as noise, and the model is learning them. Whereas
the transformer model isn't doing that, so it only fits on the more meaningful
features. 

\subsection*{Rel Sq - Squared Relative Error}

This error is sensitive to errors on far objects. However, since we are dealing
with Monodepth perception, it makes sense if this error is higher compared to
other metrics, since again, we are dealing with relative depth of each pixel to
others. \\\\
As such, for the purpose of this project, since we are only focusing on
monodepth perception, we never really paid too much attention to this value
alone. But, it being higher than the other ones is a good indication that
although our model is good at determining the relative depth of pixels, it
cannot accurately determine the true depth value. \\\\
When comparing this metric against other ones in the KITTI benchmark table, our
value of approximately 0.09 is a lot larger than the best model on the list
which is 0.006. Since that deals with stereo depth perception. 

\subsection*{Abs Rel - Absolute Relative Error}

Similar to the SiLog metric, this metric analyzes the absolute depth error
relative to the ground truth. So, it essentially looks at on average how far the
prediction is proportionally, regardless of the true depth value, which is
perfect for mono-depth perception. \\\\
This metric was used primarily to compare how this model performs relative to
the other models on the KITTI benchmark list, since even if a model on that list
uses stereo depth perception, this metric still only measures the relative depth
values instead of absolute. \\\\
To systematically determine whether a model is performing well in this case, we
want to make this value as low as possible since that implies the relative depth
of each pixel is accurate, which is what we need in mono depth perception.

\subsection*{iRMSE - Inverse Root Mean Squared Error}

This metric measures whether the model is precise on near objects and edges. It
serves the opposite function of the Squared Relative Error, which although
important for concepts like Stereo depth perception, was ignored in our case
since we are only focusing on the depth of pixels relative to each other. 



\section*{Team Contributions} \label{sec:team_contributions} Everyone in the
group contributed equally to this project. Omar Alam focused on developing the
transformer model, Nirmal Chaudhari focused on experimenting with different
ResNet model backbones, and Kevin Zhu focused on researching related works.
Everyone contributed to writing the report and editing.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only

\bibliography{custom}


% \appendix

% \section{Example Appendix} \label{sec:appendix}

% This is an appendix.

\end{document}
