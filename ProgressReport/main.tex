\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}

\setlength{\parindent}{0pt}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 27 Progress Report:\\Mono Depth Perception}


\author{Omar Alam, Nirmal Chaudhary, Kevin Zhu \\
\author{Omar Alam, Nirmal Chaudhary, Kevin Zhu \\
  \texttt{\{alamo2,chaudn12,zhuk41\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

As researchers are looking for more ways for computers to read the environment, depth perception is becoming an increasingly important feature. Traditionally, depth perception is done with LIDAR and radar. Lidar and radar-based sensors are accurate at long ranges but are more expensive compared to camera-based setups. Cameras, on the other hand do not have proper depth perception without the use of multiple lenses creating a stereo setup. Even then, stereo setups are less accurate than LiDAR or radar. 

To attempt the accuracy of LIDAR setups without needing use LIDAR, we are using deep learning techniques to create a model that can predict distances on an image. The model will be trained on taken images along with corresponding distances measured from more accurate LIDAR sources. This problem is still considered an unsolved research problem since it is extremely difficult for models to learn to estimate depth accurately enough for robotics applications using a single image only. 

If the single image depth perception is solved effectively with a robust solution, it will be impactful in many fields, with the primary use being for autonomous features in vehicles. As these vehicles need a large suite of sensors to properly detect their environment, its possible cut costs by reduce the number of sensors needed for proper autonomous function. Cheaper and more accessible depth perception would also encourage other companies to consider and integrate it into products.


\section{Related Work} \label{sec:related-work}

There are numerous groups of people and institutions that experimented with unique methods for mono depth prediction, particularly using the same KITTI dataset described in the section \href{sec:dataset}{below}. In fact, on the website that was used to retrieve this dataset, there is a leaderboard outlining which methods yield the best results \citep{KITTI}. These methods were explored by our team to understand the approaches that obtain the best result. Although the related works were used as inspiration, the purpose of this project is to experiment and find unique ways to improve on existing methods. 

Based on our findings of related works, we realized one of the key element that varies between different papers is the choice of backbone used to extract feature maps from RGB images. The backbone defines the feature space, from which depth can be inferred using a Neural Network, and therefore has a direct impact on accuracy.

Moreover, in addition to the backbone, the decoder and depth representation that is used are other key element that vary between related works. The decoder is a neural network that takes in the feature map produced by the backbone and reconstructs a dense, spatially aligned feature map (at the same resolution as the input RGB image). Following this, the depth representation head converts the decoded feature map into a final depth prediction. As such, these two components describe how to reconstruct and analyze depth using the feature map outputted by the backbone. 

While looking into related work, we realized some approaches were training a model to analyze stereo depth perception instead of mono. These approaches were ignored for the purpose of this project since the focus is on mono depth perception. But the comparison of backbones used between stereo versus mono approaches was interesting. Since, the initial assumption was that for mono depth perception a more expressive backbone would be needed since it can't use parallax method. However, this wasn't really mentioned in any of the papers. 

Some of the relevant works that were investigated include:
\begin{itemize}
  \item UniDepth: Uses a ViT-based DPT (Dense Prediction Transformer) backbone. For the decoder, it uses a DPT-style multi-scale fusion decoder. For the depth representation, it predicts a pseudo-spherical 3D representation. \citep{Piccinelli2024UniDepth}. 
  \item MSFusion: Uses a standard ResNet50-based encoder backbone to extract 2D texture features. It also converts the image into a 3D point cloud representation. It then uses a multi-space fusion decoder to fuse both into a spatial feature map. For depth representation, it outputs the actual depth prediction \citep{Bie2025MultiSpaceDepth}.
  \item DCDepth: Uses a standard ResNet or HRNet backbone to extract 2D image features from RGB image. The decoder outputs DCT frequency coefficients for each depth patch. Uses inverse DCT to transform the coefficients into a pixel-wise depth as the depth representation \citep{Wang2024DCDepth}.
  \item NDDepth: Uses a standard ResNet-based backbone to extract 2D image features from RGB image. The decoder branches into two prediction heads, the normal distance head and the direct depth head. The depth representation is the true depth in meters, which is derived by selecting which head to trust per pixel \citep{Shao2023NDDepth}.
\end{itemize}

\section{Dataset} \label{sec:dataset}

%You should write about your dataset here, following the guidelines 
%regarding item 1. This section may be 0.5-1 pages. Depending on your 
%specific dataset, you may want to include subsections for the preprocessing, 
%annotation, etc.

The mono depth perception project is using the KITTI Depth Vision Benchmark Suite. 
This dataset contains over 93,000 gray scale depth maps along with corresponding
RGB images and LiDAR scans. The LiDAR scans provide accurate depth information 
which is used to generate the ground truth depth maps. The resolution of the RGB images
is 1242 x 375 pixels with 3 color channels for red, green, and blue. The depth maps
are single channel images with the same resolution as the RGB images, where each pixel
value represents the distance from the camera to the object in meters up to a 80 meter maximum.

The following images are a single example from the dataset.
\begin{figure}[H]
  \includegraphics[width=\columnwidth]{rgb_image_0.png}
  \caption{An example RGB image from the KITTI dataset.}
  \label{fig:kitti_rgb}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\columnwidth]{depth_map_0-modified.png}
  \caption{The corresponding ground truth depth map for the RGB image in Figure~\ref{fig:kitti_rgb}. (Inverted for visualization purposes)}
  \label{fig:kitti_depth}
\end{figure}

The depth maps in the KITTI dataset have some missing values due 
to the limitations of the LiDAR sensor. The point clouds generated by 
the LiDAR tend to be sparse, especially for objects that are far away from the sensor.
Since the KITTI dataset also contains stereo images, the authors of the dataset
also use stereo reconstruction and semi-global matching. In their results,
they report achieving a better Mean Absolute Error (MAE) when combining LiDAR data
with stereo reconstruction, compared to using LiDAR data alone.

The dataset may be downloaded from the official KITTI website\footnote{http://www.cvlibs.net/datasets/kitti/eval\_depth.php}
using their provided script and is already split into training and validation sets by the authors.
To retrieve the required dataset, the raw RGB images must be downloaded
separately from the depth maps. Some preprocessing was required to remove
RGB images that did not have corresponding depth maps. 

A preliminary analysis of 1000 images from the training set shows
the following depth distribution in the dataset:
\begin{figure}[H]
  \includegraphics[width=\columnwidth]{distance_distrib.png}
  \caption{Depth distribution of 1000 images from the KITTI dataset.}
  \label{fig:depth_distribution}
\end{figure}

As shown in Figure~\ref{fig:depth_distribution}, the depth values present
in the dataset are moderately skewed towards closer objects with frequency 
decreasing as depth increases.


\section{Features} \label{sec:features}

For the initial implementation, no feature engineering or feature selection is being done. The model simply takes in an RGB image as input and processes it through a ResNet Convolutional Neural Network backbone. This transforms the pixels data into learned feature embeddings space. As such, since the backbone is an off-the-shelf CNN that learns feature representation directly from the incoming data, there is no manual feature selection needed. The exact implementation details of the backbone into the overall Neural Network model will be described in the section \href{sec:implementation}{below}. 

The use of a ResNet CNN backbone is similar to what other related works like DCDepth, MSFusion and NDDepth did as described in \href{sec:features}{section 2}, where features and correlations are dynamically extracted. The reason this works well, as opposed to manually selecting features is because the correlations between various features change a lot based on the image that is presented. As such, we need a model that captures geometric and semantic cues that are hard to generalize as hard-coded features. Evidently a CNN is needed to extract the features themselves. 

The ResNet CNN backbone and depth neural network are both a part of the same neural network, but separated as stages in the overall structure. The CNN backbone serves as the "encoder" stage of the overall neural network, where it is a CNN that outputs the learned feature embeddings space. These features are then passed into the "decoder" layer which upsamples the embeddings to return a depth map that is the same size as the original RGB image.

\section{Implementation} \label{sec:implementation}

As mentioned in the \href{sec:features}{Features} section above, the model consists of "encoder" and "decoder" layers, which is a simplification of the papers that we read in the \href{sec:related-work}{Related Work} section above. The goal of the model is to take in a single RGB image as input and predict a depth value for each pixel. Since this is mono depth perception, the accuracy of each depth value will be measured according to the relative depth in comparison to other pixels in the image. 

\subsection{Encoder}

For the initial implementation, we are using a ResNet-34 model as our encoder. This model consists of 4 successive convolutional layers that increase the channel dimensionality based on features extracted at each layer. Earlier layers focus on learning low-level patterns like edges and color gradients, while later layers focus on higher-level geometric cues. Each layer doubles the channel dimensionality, while halving the resolution. With 64, 128, 256 and 512 channels outputted from layers 1 to 4 respectively. As mentioned in the \href{sec:features}{Features} section above, we do not manually design features, instead they are learned automatically based on the image that is passed in. 

\subsection{Decoder}

The decoder in the implementation maps the learned embeddings back to pixel-aligned depth prediction, such that it is the same resolution as the original RGB input image. For the initial implementation, we use a sequence of transposed convolutional layers to upsample feature maps back to higher spatial resolution. Upsampling 4 times across all 4 layers from the backbone produces a single-channel depth map. A ReLU activation is then applied to ensure there is only non-negative depth values. 

\subsection{Data Input and Processing}

We trained and validated the model on KITTI Depth Selection Dataset, which is a subset of the entire depth prediction dataset KITTI provides. The reason we are working with a subset for the initial implementation is because we wanted to test training and evaluating the model on a subset before we download the entire dataset and require GPU clusters to train the model on all the datasets. 

Each input RGB image is resized and normalized using mean and standard deviation, as required by the ResNet-34 backbone. The ground truth (target output) depth maps are stored as 16 bit PNGs, which are converted to meters by dividing by 256. Each image and depth map is then returned as PyTorch tensors where the model is being trained as (image, depth) which represents the (input, label) pair. 

\subsection{Training Setup and Loss Function}

The model is trained using supervised learning, where the model's predicted depth map is compared to the ground-truth depth using scale-invariant loss. The reason scale-invariant loss function was used in this case, was to make it easier to benchmark this model against the other ones mentioned in the \href{sec:related-work}{Related Work} section above. 

Moreover, it is important to note that the predicted depth map is scaled using the target depth map to prevent large errors caused by the actual depth prediction values being off. Since this project focuses on mono depth perception, and not stereo, our goal is not to get the true depth in meters, but rather to determine whether the relative depth of different pixels is the same as the target.

The model is trained using stochastic gradient descent, training on a batch of 4 images at a time until all 1000 images have been passed through the model. This process is repeated for the set number of epochs defined in the code. 

During backpropagation, the encoder and decoder weights are updated, allowing the network to update it's learned feature embeddings. Since the system is trained together with encoder and decoder actively involved, the backbone's learned features also start to become more specialized for depth prediction rather than classification (which is what it was originally meant for). 

The trained model is then stored as a .pth file, with the model's trained weights. 

You can access the full implementation at our \href{https://github.com/alamo3/SFWRENG-4AL3-Group-27/tree/main}{GitHub Repository}.

\section{Results and Evaluation} \label{sec:results_and_evaluation}

%How are you evaluating your model? What results do 
%you have so far? What are your baselines? Refer to item 5. 
%This may take around 0.5 pages.

The loss function used to train the model is the Scale Invariant Loss, which was first defined 
by Eigen et al. in their 2014 paper "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network".

It is defined as follows:
\[
  D(y, y^*) = \frac{1}{n} \sum_{i=0}^{n} d_{i}^2 - \frac{1}{n^2} (\sum_{i=0}^{n} d_i)^2
\]

Where $d_i = \log y_i - \log y_i^*$, $y_i$ is the predicted depth, $y_i^*$ is the ground truth depth, and $n$ is the number of pixels in the depth map.

This function is particularly useful for evaluating the quality of the depth estimation 
since it focuses on the relative differences between predicted and ground truth depths,
rather than absolute differences. This is important since depth predictions can vary
by a scale factor as long as the relative distances between objects are preserved.
Ex: If the predicted depth is consistently twice the ground truth depth, the scale invariant loss will be low, indicating that the model has learned
the relative depth relationships correctly.

In addition to using SiLog, the following metrics are also used:
\begin{enumerate}
  \item Absolute Relative Difference (Abs Rel):
  \[
    \text{Abs Rel} = \frac{1}{n} \sum_{i=0}^{n} \frac{|y_i - y_i^*|}{y_i^*}
  \]

  \item Relative Squared Difference (Rel Sq):
  \[
    \text{Rel Sq} = \frac{1}{n} \sum_{i=0}^{n} \frac{(y_i - y_i^*)^2}{(y_i^*)^2}
  \]

  \item Inverse Root Mean Squared Error (iRMSE):
  \[
    \text{iRMSE} = \sqrt{\frac{1}{n} \sum_{i=0}^{n} \left(\frac{1}{y_i} - \frac{1}{y_i^*}\right)^2}
  \]
\end{enumerate}
These metrics provide a comprehensive evaluation of the model's performance.
The iRMSE metric is particularly useful for determining depth accuracy for closer objects.
It penalizes large errors in depth estimation for nearby objects more heavily than for distant objects,
since it is based on the inverse of depth values.

The dataset is already split into training and validation sets by the authors of the KITTI dataset as 
mentioned in the \href{sec:dataset}{Dataset} section above with a roughly 80/20 split.
There is no cross-validation performed due to the sufficiently large size of the dataset.
However, this experiment was conducted with a 
very small subset of the entire dataset, using only 1000 images for training due to 
computational constraints. 

\section{Feedback and Plans} \label{sec:feedback_and_plans}

%Write about your plans for the remainder of the project. This should include a discussion of the feedback you received from your TA, and how you plan to improve your approach. Reflect on your implementation and areas for improvement. Refer to item 6. This may be around 0.5 pages.

Our first feedback was received during a tutorial when our group proposed to the TA. The TA gave us two pieces of feedback. To look into and base our work off what other work have been done on the subject, and use a Convolutional Neural Network. 

In a call with a TA. They recommended that we should add distribution of distances in the dataset, and a baseline for naive random guess, and maybe a multi layer perceptron (simple model)

Our group knew that there was a lot of previous work done on this subject. We used this to influence how we wanted to approach this problem which was shown in our \href{sec:related-work}{Related Work} section. 

The TAs also mentioned two different methods of implementing our model with both CNNs and MLPs. Our group decide that we wanted to use a CNN specifically ResNet-34 as it was designed to be used for computer vision. However, our group will also use a MLP to use as a baseline and compare to other models 

The last bit of feedback is related to creating a baseline for naive random guess. This is something our group will do as to have a proper comparison for how good our model is. 

Some future plans of our project are to use a larger subset of the KITTI Dataset, as we are currently using a subset. To do this we would use the McMaster servers for computation as the entire dataset is bigger than 100gb and using our own devices would be unreasonable. We will also test different baselines (such as naive guesses and MLPs) and explore other models and different backbones and model configurations (including transformers).




\nocite{KITTI, Piccinelli2024UniDepth, Bie2025MultiSpaceDepth, Wang2024DCDepth, Shao2023NDDepth}

\section*{Team Contributions} \label{sec:team_contributions}

Write in this section a few sentences describing the contributions of each team member. What did each member work on? Refer to item 7.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
